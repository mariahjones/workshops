{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.stats as stats\n",
    "import astropy.stats as astats\n",
    "import numpy.random as random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting for the Hubble constant with supernovae\n",
    "\n",
    "In this notebook, we are going to learn to use linear regression to fit the hubble constant, H0, using Hubble's law:\n",
    "\n",
    "$v \\approx cz = H_0D$\n",
    "\n",
    "We are going to work with the Hicken et al. (\"Constitution\") supernova dataset.  First, we read in the file into a pandas dataframe named `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THE BELOW LINE TO POINT TO THE DIRECTORY CONTAINING SNDATA.TXT\n",
    "path = ''  \n",
    "\n",
    "# the pandas way: the file is in \"fixed-width format\" so we use read_fwf\n",
    "data=pd.read_fwf(path+'sndata.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, let's open up the data and take a look at the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data is stored with cz, a variable we would like to use, but the distances are in terms of the distance modulus, mu, which is related to the distance d in parsecs as follows:\n",
    "\n",
    "$log_{10}(d) = 1+\\frac{\\mu}{5}$\n",
    "\n",
    "Solving for d:\n",
    "\n",
    "$d = 10^{1+\\frac{\\mu}{5}}$\n",
    "\n",
    "The Hubble constant is often measured in units of km/s/Mpc, so we'll be converting everything to fit our slope in those units.\n",
    "\n",
    "Let's read in the data and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cz=data['cz'] #already in km/s\n",
    "mu=data['mu']\n",
    "sigma_mu=data['sigma_mu']\n",
    "d = 10**(1+mu/5) * 1e-6 #convert to Mpc\n",
    "\n",
    "plt.plot(cz,d, 'b.')\n",
    "plt.xlabel('cz (km/s)')\n",
    "plt.ylabel(r'Distance (Mpc)')\n",
    "plt.ylim(0,500)\n",
    "plt.show()\n",
    "\n",
    "plt.errorbar(cz,mu,yerr=sigma_mu,fmt='b.')\n",
    "plt.xlabel('cz (km/s)')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our measurements are made on $\\mu$ and therefore our errors are in $\\mu$, we're going to fit in logarithmic space. This is extremely common in astronomy, as our measurements often span orders of magnitude in brightness/distance.\n",
    "\n",
    "$d = \\frac{cz}{H_0} x 10^6$ parsecs\n",
    "\n",
    "$log_{10}(d) = 1+\\frac{\\mu}{5} = log_{10}(\\frac{cz}{H_0})+ 6$\n",
    "\n",
    "$\\mu = 5 log_{10}(cz) + 5[5-log_{10}(H_0)]$\n",
    "\n",
    "So, if we fit a line to this, the intercept, b, will be:\n",
    "\n",
    "$b = 5[5-log_{10}(H_0)]$\n",
    "\n",
    "And $H_0$ will be:\n",
    "\n",
    "$H_0 = 10^5 10^{-0.2*b} \\mathrm{\\frac{km/s}{Mpc}}$\n",
    "\n",
    "<b>In the cell below, plot $\\mu$ as a function of $\\mathrm{log_{10}(cz)}$. Name the variable for the log of cz \"logv\":</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's fit a line to this data and get $H_0$ from the intercept. We can use the scipy.stats linregress function to do this. If this worked properly, our slope should be very close to 5, so we can print that as a check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to convert from the intercept to H0 to make things easier\n",
    "def int_to_H0(b):\n",
    "    return(10**(-0.2*b) * 10**5)\n",
    "\n",
    "slope,intercept,r,p,s = stats.linregress(logv,mu)\n",
    "\n",
    "plt.plot(logv,mu,'b.')\n",
    "plt.ylim(33,39)\n",
    "plt.xlabel('log v')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "plt.plot(logv,logv*slope+intercept,'r-')\n",
    "print('slope: ',slope,' +/- ',s)\n",
    "print('intercept: ',intercept)\n",
    "print('H0: ',int_to_H0(intercept))\n",
    "\n",
    "mu_fit = slope*logv+intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got $H_0$ as very close to 70, which is very close to the accepted value! However, we didn't incorporate our errors into our measurement at all. In order to do that, let's do an inverse variance weighted least-squares instead. We can use the numpy polyfit routine to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 1./sigma_mu**2\n",
    "\n",
    "#call polyfit with our inverese variance weights and fit a 1st order (linear) polynomial\n",
    "coeffs, covar = np.polyfit(logv,mu,1,w=weight,cov=True)\n",
    "slope = coeffs[0]\n",
    "intercept = coeffs[1]\n",
    "intercept_err = np.sqrt(covar[1,1])\n",
    "s=np.sqrt(covar[0,0])\n",
    "\n",
    "print('slope: ',slope,' +/- ',s)\n",
    "print('intercept: ',intercept)\n",
    "\n",
    "#to get the error in H0 from this method, take the average of H0 from the intercept +1 sigma and -1 sigma errors\n",
    "h0err = (int_to_H0(intercept-intercept_err)-int_to_H0(intercept+intercept_err))/2\n",
    "print('H0: ',int_to_H0(intercept),'+/-',h0err)\n",
    "\n",
    "plt.errorbar(logv,mu,yerr=sigma_mu,fmt='b.')\n",
    "plt.ylim(33,39)\n",
    "plt.xlabel('log v')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "plt.plot(logv,logv*slope+intercept,'r-')\n",
    "plt.show()\n",
    "\n",
    "mu_fit_err = slope*logv+intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate how good a fit is, we use a parameter called $\\chi^2$:\n",
    "\n",
    "$\\chi^2 = \\Sigma_i \\frac{(y_i-f(x_i, \\beta))^2}{\\sigma_i^2}$\n",
    "\n",
    "$f(x_i, \\beta)$ is our model in this case, which is a function of x (in our case, log(v)) and some parameters $\\beta$, in our case the slope and the intercept.\n",
    "\n",
    "The goal of regression is to minimize $\\chi^2$; the model which gives the lowest value is the best fit. In the above cases, the weighted and unweighted methods are the same, the unweighted just sets sigma=1 for all points so that they have equal weight in the sum.\n",
    "\n",
    "In the below cell, calculate $\\chi^2$ for the inverse variance weighted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_linear = np.sum(?)\n",
    "print('chi2: ', chi2_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above case, we assumed that Hubble's law had a linear form and found a good fit to the data, but that doesn't have to be the case. What if instead, there's both linear and quadratic dependence on v? Let's try doing that fit. Instead of evaluating by hand, we can use the numpy polyval function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs, covar = np.polyfit(logv,mu,2,w=weight,cov=True)\n",
    "print('quadratic coefficients', coeffs)\n",
    "# note the order of the coefficients: the quadratic term is first, constant last\n",
    "\n",
    "quad_fit = np.polyval(coeffs,logv)\n",
    "\n",
    "poly_quad = np.poly1d(coeffs)\n",
    "quad_fit2  = poly_quad(logv)\n",
    "\n",
    "#make a plot with the linear and quadtratic lines plotted along with the points\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that both of these fits perform very similarly, but can we assess which model actually fits better? Find $\\chi^2$ for the quadratic fit and see which fit minimizes $\\chi^2$ more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the quadratic chi^2 and print it, along with the linear one\n",
    "???\n",
    "\n",
    "print(chi2_linear, chi2_quad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that the quadratic model outperforms the linear one. That <b>has to</b> be the case, because we've just taken the linear model and added more parameters. Adding more parameters to an existing model will <b>always</b> improve $\\chi^2$, or leave it basically unchanged. But in that case, why not just fit a 1000th order polynomial every time?\n",
    "\n",
    "There are a few answers here. The first is that we want our models to be physically motivated so that they can extrapolate. Let's take a look in the below cell at how our two models look when we go outside the range of our data, and let's also add third, fourth, and 50th order fits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_lin, covar_lin = np.polyfit(logv,mu,1,w=weight,cov=True)\n",
    "coeffs_quad, covar_quad = ?\n",
    "coeffs_cub, covar_cub = ?\n",
    "coeffs_quar, covar_quar = ?\n",
    "coeffs_50 = np.polyfit(logv,mu,50,w=weight,cov=False) #can't do covariance estimation for this high order\n",
    "\n",
    "#define a larger range of logv than our data\n",
    "logv_arr = np.linspace(1,7, 100)\n",
    "\n",
    "plt.errorbar(logv,mu,yerr=sigma_mu,fmt='b.')\n",
    "plt.plot(logv_arr, np.polyval(coeffs_lin, logv_arr), label='Linear')\n",
    "plt.plot(logv_arr, np.polyval(coeffs_quad, logv_arr), label='Quadratic')\n",
    "plt.plot(logv_arr, np.polyval(coeffs_cub, logv_arr), label='Cubic')\n",
    "plt.plot(logv_arr, np.polyval(coeffs_quar, logv_arr), label='Quartic')\n",
    "plt.plot(logv_arr, np.polyval(coeffs_50, logv_arr), label='50th Order')\n",
    "plt.ylim(25,48)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all the models do a good job of fitting the data in the range of interest, but the way they extrapolate has serious differences. As a result, we need to be extremely cautious when we extend our models past the range of our data. If we don't have a physically motivated model, extrapolation is very dangerous, and even if we do, we still need to be careful. The best practice in this case would be to get more data at higher redshift and then fit again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That being said, we can make some statements about whether or not we're overfitting using only the data in our range of interest. For that, we turn to measures like the Akaike Information Criterion or the Bayesian Information Criterion:\n",
    "\n",
    "$BIC = -2~\\ln{L_{max}} + k\\ln{n} = \\chi^2 + k \\ln{n}$\n",
    "\n",
    "$AIC = -2~\\ln{L_{max}} + 2 k = \\chi^2 +2 k$\n",
    "\n",
    "In both cases, k is the number of free parameters in the model and for BIC, n is the number of data points that we have. Let's evaluate both BIC and AIC for our linear and quadratic models. AIC and BIC are improved if they <b>decrease</b> significantly. If they increase as we add more parameters, we are absolutely overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_lin = 2\n",
    "k_quad = 3\n",
    "\n",
    "bic_linear = chi2_linear+k_lin*np.log(len(logv))\n",
    "bic_quad = chi2_quad+k_quad*np.log(len(logv))\n",
    "\n",
    "print('BIC: ')\n",
    "print(bic_linear, bic_quad)\n",
    "\n",
    "aic_linear = chi2_linear+2*k_lin\n",
    "aic_quad = chi2_quad+2*k_quad\n",
    "print('AIC: ')\n",
    "print(aic_linear, aic_quad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, we increase our information criteria by adding more parameters, and as such, we should treat our linear fit as the best one. For the differences between AIC and BIC and advice about when to use which, bug Jeff Newman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Linear Regression with Curve Fit\n",
    "=============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure if we'll have time to go over this in detail, but we are absolutely not restricted to fitting polynomial models to our data, and that is rarely actually the most interesting thing to do. A lot of the time in astronomy, we're interested in emission features, which can be modeled reasonably well with Gaussians. In my research, I think a lot about the [OIII] doublet, which features lines that emit at 5007 and  4959 angstroms with a theoretical 3:1 flux ratio.\n",
    "\n",
    "Below, we're going to fit a Gaussian to some data from an SDSS galaxy where I have subtracted out the contribution from stars, leaving behind just the emission feature. The wavelength and flux are contained in emline.txt. The model of the gaussian we will be fitting to the doublet will take the following form:\n",
    "\n",
    "$f(\\lambda) = a~(e^\\frac{-(\\lambda-\\mu)^2}{\\sigma^2}+\\frac{1}{3}~e^\\frac{-(\\lambda-\\mu+48)^2}{\\sigma^2})$\n",
    "\n",
    "a is the absolute normalization, $\\mu$ is the median of the stronger line in the doublet, and sigma is the dispersion of the gaussian. The second line is fixed to be 1/3 the peak of the first and to be 48 angstroms blueward.\n",
    "\n",
    "In the below cell, let's read in the emission line data and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emline = np.genfromtxt('emline.txt')\n",
    "wl = emline[:,0]\n",
    "flux = emline[:,1]\n",
    "\n",
    "plt.plot(wl, flux)\n",
    "plt.xlabel('Wavelength (Angstroms)')\n",
    "plt.ylabel('Flux')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's noise, but we can definitely wee two strong peaks centered at the theoretical wavelenghts. Next, let's fit our model to the data and plot using curve fit. Curve fit takes a function that we want to fit to with an arbitrary number of parameters, as well as x, y, and an initial guess. Unlike linear least squares, nonlinear least squares does not necessarily converge. As such, we need to make sure we give a reasonable set of guesses so that the parameters are in a part of parameter space they can find a solution. Those are given in p0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def gaussian_OIII(lam, a, mu, sigma):\n",
    "    ???\n",
    "    \n",
    "params, covar = curve_fit(gaussian_OIII, wl, flux, p0=[1, 5007, 10])\n",
    "a, mu, sigma = params\n",
    "\n",
    "plt.plot(wl, flux)\n",
    "plt.plot(wl, gaussian_OIII(wl, a, mu, sigma))\n",
    "plt.xlabel('Wavelength (Angstroms)')\n",
    "plt.ylabel('Flux')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good! However, you might notice that there's a secondary peak that appears in both of the lines. Sometimes, in studying emission lines, we like to throw multiple gaussians onto a line and see if we can identify blueshifted or redshifted components that could be the result of outflows, mergers, or other weird stuff going on with the gas. In the below cell, try fitting a two Gaussian model to the line and see if you can capture that blueshifted peak.\n",
    "\n",
    "$f(\\lambda) = a~(e^\\frac{-(\\lambda-\\mu_1)^2}{\\sigma_1^2}+\\frac{1}{3}~e^\\frac{-(\\lambda-\\mu_1+48)^2}{\\sigma_1^2})\n",
    "~+~ b~(e^\\frac{-(\\lambda-\\mu_2)^2}{\\sigma_2^2}+\\frac{1}{3}~e^\\frac{-(\\lambda-\\mu_2+48)^2}{\\sigma_2^2})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_OIII_2comp(lam,a,mu1,sigma1,b,mu2,sigma2):\n",
    "    ???\n",
    "    \n",
    "params, covar = curve_fit(gaussian_OIII_2comp, wl, flux, p0=[1, 5007, 10, 1, 5007, 10])\n",
    "a, mu1, sigma1, b, mu2, sigma2 = params\n",
    "\n",
    "plt.plot(wl_line, flux_line)\n",
    "plt.plot(wl_line, gaussian_OIII_2comp(wl, a, mu1, sigma1, b, mu2, sigma2))\n",
    "plt.xlabel('Wavelength (Angstroms)')\n",
    "plt.ylabel('Flux')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the ability to define our own model in curve fit opens up a ton of fitting possibilities. The trick is coming up with a way to find our parameters, along with the errors in those parameters. In the next lesson, we'll focus on one method of determining the errors in our measurements: bootstrap resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
